# LiteLLM Proxy for AWS EKS
# Uses AWS Bedrock via IRSA (no static credentials)
# Database: Aurora PostgreSQL, Cache: ElastiCache Redis (TLS)

replicaCount: 1

image:
  repository: ghcr.io/berriai/litellm-database
  pullPolicy: IfNotPresent

# Service account for Bedrock access (Pod Identity association managed by Terraform)
serviceAccount:
  create: true

podAnnotations:
  reloader.stakater.com/auto: "true"

resources:
  requests:
    cpu: 250m
    memory: 500Mi
  limits:
    memory: 1Gi

# Database config - secrets injected via secretFrom
db:
  deployStandalone: false
  useExisting: false

# Inject litellm K8s secret (DATABASE_URL, PROXY_MASTER_KEY, LITELLM_SALT_KEY)
secretFrom:
  name: litellm

# Environment variables (ConfigMap)
env:
  LITELLM_LOG: "INFO"
  LITELLM_MODE: PRODUCTION
  REDIS_URL: "rediss://master.redis-uq-dogfood-sbx-euc2.jwixqt.euc2.cache.amazonaws.com:6380/0"

# Migration job for Prisma schema
hooks:
  migration:
    enabled: true
    command: |
      python litellm/proxy/prisma_migration.py

# Proxy configuration
proxy_config:
  model_list:
    # Anthropic Claude models via AWS Bedrock cross-region inference profiles
    # Uses eu.* inference profile IDs (required for on-demand invocation)
    - model_name: anthropic-claude-sonnet-4-5
      litellm_params:
        model: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: anthropic-claude-opus-4-5
      litellm_params:
        model: bedrock/eu.anthropic.claude-opus-4-5-20251101-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: anthropic-claude-opus-4-6
      litellm_params:
        model: bedrock/eu.anthropic.claude-opus-4-6-v1
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: anthropic-claude-haiku-4-5
      litellm_params:
        model: bedrock/eu.anthropic.claude-haiku-4-5-20251001-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: anthropic-claude-3-5-sonnet
      litellm_params:
        model: bedrock/eu.anthropic.claude-3-5-sonnet-20240620-v1:0
        aws_region_name: eu-central-2
        max_tokens: 8192

    - model_name: anthropic-claude-3-haiku
      litellm_params:
        model: bedrock/eu.anthropic.claude-3-haiku-20240307-v1:0
        aws_region_name: eu-central-2
        max_tokens: 4096

    # OpenAI model name aliases (for chat service Azure SDK compatibility)
    # Maps OpenAI model names to Bedrock Claude models via LiteLLM
    - model_name: gpt-4o
      litellm_params:
        model: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: gpt-4o-2024-11-20
      litellm_params:
        model: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: gpt-4-turbo
      litellm_params:
        model: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    - model_name: gpt-4-32k
      litellm_params:
        model: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
        aws_region_name: eu-central-2
        max_tokens: 10000

    # Embedding model alias (for ingestion service Azure OpenAI SDK compatibility)
    # Maps text-embedding-ada-002 to Cohere Embed v4 (1536 dims)
    # Note: embedding models don't support cross-region inference profiles (eu.* prefix)
    - model_name: text-embedding-ada-002
      litellm_params:
        model: bedrock/cohere.embed-v4:0
        aws_region_name: eu-central-2

  general_settings:
    master_key: os.environ/PROXY_MASTER_KEY
    alerting: []
    proxy_batch_write_at: 60
    database_connection_pool_limit: 10
    disable_spend_logs: false
    disable_error_logs: false

  litellm_settings:
    drop_params: true
    turn_off_message_logging: false
    max_budget: 10000
    budget_duration: 30d
    json_logs: true
    cache: true
    cache_params:
      type: redis
      namespace: "litellm.caching.caching"
    redact_messages_in_exceptions: true
    request_timeout: 600
    set_verbose: false

securityContext:
  allowPrivilegeEscalation: false

ingress:
  enabled: false
